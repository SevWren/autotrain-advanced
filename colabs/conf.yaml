
task: llm-sft
base_model: abhishek/llama-2-7b-hf-small-shards
project_name: my-autotrain-llm-rpgquest
log: tensorboard
backend: local

data:
  path: data/
  train_split: train
  valid_split: null
  chat_template: null
  column_mapping:
    text_column: text

params:
  block_size: 512
  lr: 0.0003
  warmup_ratio: 0.1
  weight_decay: 0.01
  epochs: 5
  batch_size: 8
  gradient_accumulation: 2
  mixed_precision: fp16
  peft: True
  quantization: int4
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  unsloth: False

hub:
  username: ${HF_USERNAME}
  token: ${HF_TOKEN}
  push_to_hub: False
